{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b956f8f9-a4ec-48bf-ae6b-9bf68f66fac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before removing outliers: (9639, 19)\n",
      "Shape after removing outliers: (5636, 19)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"D:\\Sem 5\\projects\\FDS\\liver_cirrhosis.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_iqr(data, columns):\n",
    "    for column in columns:\n",
    "        Q1 = data[column].quantile(0.25)  # 25th percentile\n",
    "        Q3 = data[column].quantile(0.75)  # 75th percentile\n",
    "        IQR = Q3 - Q1                   # Interquartile range\n",
    "        lower_bound = Q1 - 1.5 * IQR    # Lower bound\n",
    "        upper_bound = Q3 + 1.5 * IQR    # Upper bound\n",
    "        \n",
    "        # Filter the data\n",
    "        data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "    return data\n",
    "\n",
    "# Apply the function to numeric columns\n",
    "numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Save the shape before removing outliers\n",
    "shape_before = data.shape\n",
    "\n",
    "# Remove outliers\n",
    "data = remove_outliers_iqr(data, numeric_columns)\n",
    "\n",
    "# Print the shapes\n",
    "print(f\"Shape before removing outliers: {shape_before}\")\n",
    "print(f\"Shape after removing outliers: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd96b90a-49b8-4cf2-8589-c63c80a89a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.73      0.73       617\n",
      "           2       0.69      0.70      0.70       640\n",
      "           3       0.82      0.82      0.82       671\n",
      "\n",
      "    accuracy                           0.75      1928\n",
      "   macro avg       0.75      0.75      0.75      1928\n",
      "weighted avg       0.75      0.75      0.75      1928\n",
      "\n",
      "Model: Decision Tree | Accuracy: 0.7505\n",
      "ROC AUC Score: 0.8172106191346468\n",
      "Confusion Matrix:\n",
      "[[448 123  46]\n",
      " [119 448  73]\n",
      " [ 44  76 551]]\n",
      "========================================\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.86      0.85      0.86       617\n",
      "           2       0.84      0.84      0.84       640\n",
      "           3       0.91      0.91      0.91       671\n",
      "\n",
      "    accuracy                           0.87      1928\n",
      "   macro avg       0.87      0.87      0.87      1928\n",
      "weighted avg       0.87      0.87      0.87      1928\n",
      "\n",
      "Model: Random Forest | Accuracy: 0.8703\n",
      "ROC AUC Score: 0.9620011782466382\n",
      "Confusion Matrix:\n",
      "[[527  72  18]\n",
      " [ 57 540  43]\n",
      " [ 27  33 611]]\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venka\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Bagging Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.84      0.82       617\n",
      "           2       0.81      0.77      0.79       640\n",
      "           3       0.89      0.89      0.89       671\n",
      "\n",
      "    accuracy                           0.84      1928\n",
      "   macro avg       0.83      0.83      0.83      1928\n",
      "weighted avg       0.84      0.84      0.83      1928\n",
      "\n",
      "Model: Bagging Classifier | Accuracy: 0.8351\n",
      "ROC AUC Score: 0.9400717001017549\n",
      "Confusion Matrix:\n",
      "[[521  73  23]\n",
      " [ 98 495  47]\n",
      " [ 32  45 594]]\n",
      "========================================\n",
      "Model: Gradient Boosting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.79      0.79       617\n",
      "           2       0.74      0.75      0.75       640\n",
      "           3       0.86      0.84      0.85       671\n",
      "\n",
      "    accuracy                           0.79      1928\n",
      "   macro avg       0.79      0.79      0.79      1928\n",
      "weighted avg       0.80      0.79      0.79      1928\n",
      "\n",
      "Model: Gradient Boosting | Accuracy: 0.7946\n",
      "ROC AUC Score: 0.9305219518220097\n",
      "Confusion Matrix:\n",
      "[[489 100  28]\n",
      " [ 92 482  66]\n",
      " [ 39  71 561]]\n",
      "========================================\n",
      "Model: Support Vector Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.75      0.70       617\n",
      "           2       0.67      0.58      0.62       640\n",
      "           3       0.80      0.79      0.79       671\n",
      "\n",
      "    accuracy                           0.71      1928\n",
      "   macro avg       0.71      0.71      0.70      1928\n",
      "weighted avg       0.71      0.71      0.71      1928\n",
      "\n",
      "Model: Support Vector Classifier | Accuracy: 0.7070\n",
      "ROC AUC Score: 0.8701504894508272\n",
      "Confusion Matrix:\n",
      "[[462 111  44]\n",
      " [179 371  90]\n",
      " [ 66  75 530]]\n",
      "========================================\n",
      "Model: K-Nearest Neighbors\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.78      0.73       617\n",
      "           2       0.70      0.66      0.68       640\n",
      "           3       0.85      0.79      0.82       671\n",
      "\n",
      "    accuracy                           0.74      1928\n",
      "   macro avg       0.74      0.74      0.74      1928\n",
      "weighted avg       0.75      0.74      0.74      1928\n",
      "\n",
      "Model: K-Nearest Neighbors | Accuracy: 0.7427\n",
      "ROC AUC Score: 0.8821265395827916\n",
      "Confusion Matrix:\n",
      "[[484 105  28]\n",
      " [153 421  66]\n",
      " [ 70  74 527]]\n",
      "========================================\n",
      "Model: Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.75      0.58       617\n",
      "           2       0.45      0.28      0.35       640\n",
      "           3       0.68      0.57      0.62       671\n",
      "\n",
      "    accuracy                           0.53      1928\n",
      "   macro avg       0.54      0.53      0.51      1928\n",
      "weighted avg       0.54      0.53      0.52      1928\n",
      "\n",
      "Model: Naive Bayes | Accuracy: 0.5290\n",
      "ROC AUC Score: 0.7226009242707908\n",
      "Confusion Matrix:\n",
      "[[460  94  63]\n",
      " [348 179 113]\n",
      " [169 121 381]]\n",
      "========================================\n",
      "Model: Neural Network\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.72      0.72       617\n",
      "           2       0.68      0.68      0.68       640\n",
      "           3       0.81      0.80      0.80       671\n",
      "\n",
      "    accuracy                           0.74      1928\n",
      "   macro avg       0.74      0.73      0.73      1928\n",
      "weighted avg       0.74      0.74      0.74      1928\n",
      "\n",
      "Model: Neural Network | Accuracy: 0.7360\n",
      "ROC AUC Score: 0.8850198093051294\n",
      "Confusion Matrix:\n",
      "[[445 125  47]\n",
      " [120 438  82]\n",
      " [ 51  84 536]]\n",
      "========================================\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001128 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2385\n",
      "[LightGBM] [Info] Number of data points in the train set: 8004, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "Model: LightGBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.90      0.90       617\n",
      "           2       0.88      0.88      0.88       640\n",
      "           3       0.93      0.94      0.94       671\n",
      "\n",
      "    accuracy                           0.91      1928\n",
      "   macro avg       0.91      0.91      0.91      1928\n",
      "weighted avg       0.91      0.91      0.91      1928\n",
      "\n",
      "Model: LightGBM | Accuracy: 0.9072\n",
      "ROC AUC Score: 0.9773014859080947\n",
      "Confusion Matrix:\n",
      "[[553  53  11]\n",
      " [ 41 563  36]\n",
      " [ 13  25 633]]\n",
      "========================================\n",
      "Model: CatBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.89      0.89       617\n",
      "           2       0.86      0.87      0.87       640\n",
      "           3       0.92      0.92      0.92       671\n",
      "\n",
      "    accuracy                           0.89      1928\n",
      "   macro avg       0.89      0.89      0.89      1928\n",
      "weighted avg       0.89      0.89      0.89      1928\n",
      "\n",
      "Model: CatBoost | Accuracy: 0.8942\n",
      "ROC AUC Score: 0.9713713225613768\n",
      "Confusion Matrix:\n",
      "[[552  49  16]\n",
      " [ 48 557  35]\n",
      " [ 18  38 615]]\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"D:\\\\Sem 5\\\\projects\\\\FDS\\\\liver_cirrhosis.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert categorical columns to category codes\n",
    "for col in data.select_dtypes(include=['object']).columns:\n",
    "    data[col] = data[col].astype('category').cat.codes\n",
    "\n",
    "# Define the target column\n",
    "target_column = 'stage'\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop([target_column], axis=1)\n",
    "y = data[target_column]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Predict probabilities for models that support it\n",
    "    y_proba = model.predict_proba(X_test) if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model: {name} | Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr')\n",
    "            print(\"ROC AUC Score:\", roc_auc)\n",
    "        except ValueError as e:\n",
    "            print(\"Error calculating ROC AUC:\", e)\n",
    "    else:\n",
    "        print(\"ROC AUC Score: N/A\")\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "# Define the models (including XGBoost, LightGBM, CatBoost, and BaggingClassifier)\n",
    "models = [\n",
    "    (\"Decision Tree\", Pipeline([('scaler', scaler), ('classifier', DecisionTreeClassifier(class_weight='balanced'))])),\n",
    "    (\"Random Forest\", Pipeline([('scaler', scaler), ('classifier', RandomForestClassifier(random_state=42))])),\n",
    "    (\"Bagging Classifier\", Pipeline([('scaler', scaler), ('classifier', BaggingClassifier(base_estimator=DecisionTreeClassifier(), random_state=42))])),\n",
    "    (\"Gradient Boosting\", Pipeline([('scaler', scaler), ('classifier', GradientBoostingClassifier(random_state=42))])),\n",
    "    (\"Support Vector Classifier\", Pipeline([('scaler', scaler), ('classifier', SVC(probability=True, random_state=42))])),\n",
    "    (\"K-Nearest Neighbors\", Pipeline([('scaler', scaler), ('classifier', KNeighborsClassifier())])),\n",
    "    (\"Naive Bayes\", Pipeline([('scaler', scaler), ('classifier', GaussianNB())])),\n",
    "    (\"Neural Network\", Pipeline([('scaler', scaler), ('classifier', MLPClassifier(random_state=42, max_iter=1000))])),\n",
    "    (\"LightGBM\", Pipeline([('scaler', scaler), ('classifier', lgb.LGBMClassifier(objective='multiclass', random_state=42))])),\n",
    "    (\"CatBoost\", Pipeline([('scaler', scaler), ('classifier', cb.CatBoostClassifier(iterations=1000, learning_rate=0.05, depth=10, random_state=42, verbose=0))])),\n",
    "]\n",
    "\n",
    "# Loop through models and evaluate\n",
    "for name, model in models:\n",
    "    evaluate_model(name, model, X_train_resampled, X_test, y_train_resampled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265317cb-6d21-40d7-8ec5-c1d1e76bf792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
